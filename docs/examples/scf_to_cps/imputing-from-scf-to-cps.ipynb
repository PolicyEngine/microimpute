{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993697e6",
   "metadata": {},
   "source": [
    "# Imputing wealth from the SCF to the CPS\n",
    "\n",
    "This notebook demonstrates how to use the `microimpute` package and specifically the `autoimpute` function to impute wealth variables from the Survey of Consumer Finances to the Current Population Survey.\n",
    "\n",
    "The Survey of Consumer Finances (SCF) is a triennial survey conducted by the Federal Reserve that collects detailed information on U.S. families' balance sheets, income, and demographic characteristics, with a special focus on wealth measures. The Current Population Survey (CPS) is a monthly survey conducted by the Census Bureau that provides comprehensive data on the labor force, employment, unemployment, and demographic characteristics, but lacks detailed wealth information.\n",
    "\n",
    "By using `microimpute`, wealth information can be transfered from the SCF to the CPS, enabling economic analyses that require both detailed labor market and wealth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c672d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import logging\n",
    "import zipfile\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pydantic import validate_call\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "from microimpute.config import (\n",
    "    VALIDATE_CONFIG, VALID_YEARS, PLOT_CONFIG\n",
    ")\n",
    "from microimpute.comparisons import *\n",
    "from microimpute.visualizations import *\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5459dc7",
   "metadata": {},
   "source": [
    "## Loading and preparing the SCF and CPS datasets\n",
    "\n",
    "The first step in the imputation process involves acquiring and harmonizing the two datasets. Extracting data from the SCF and the CPS, and then processing it to ensure the variables are compatible for imputation are crucial pre-processing steps for successful imputation. This involves identifying predictor variables that exist in both data sets and can meaningfully predict wealth, as well as ensuring they are named and encoded identically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba8f2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@validate_call(config=VALIDATE_CONFIG)\n",
    "def scf_url(year: int, VALID_YEARS: List[int] = VALID_YEARS) -> str:\n",
    "    \"\"\"Return the URL of the SCF summary microdata zip file for a year.\n",
    "\n",
    "    Args:\n",
    "        year: Year of SCF summary microdata to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        URL of summary microdata zip file for the given year.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the year is not in VALID_YEARS.\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Generating SCF URL for year {year}\")\n",
    "\n",
    "    if year not in VALID_YEARS:\n",
    "        logger.error(\n",
    "            f\"Invalid SCF year: {year}. Valid years are {VALID_YEARS}\"\n",
    "        )\n",
    "        raise ValueError(\n",
    "            f\"The SCF is not available for {year}. Valid years are {VALID_YEARS}\"\n",
    "        )\n",
    "\n",
    "    url = f\"https://www.federalreserve.gov/econres/files/scfp{year}s.zip\"\n",
    "    logger.debug(f\"Generated URL: {url}\")\n",
    "    return url\n",
    "\n",
    "\n",
    "@validate_call(config=VALIDATE_CONFIG)\n",
    "def load_scf(\n",
    "    years: Optional[Union[int, List[int]]] = VALID_YEARS,\n",
    "    columns: Optional[List[str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Load Survey of Consumer Finances data for specified years and columns.\n",
    "\n",
    "    Args:\n",
    "        years: Year or list of years to load data for.\n",
    "        columns: List of column names to load.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing the requested data.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no Stata files are found in the downloaded zip\n",
    "            or invalid parameters\n",
    "        RuntimeError: If there's a network error or a problem processing\n",
    "            the downloaded data\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(f\"Loading SCF data with years={years}\")\n",
    "\n",
    "    try:\n",
    "        # Identify years for download\n",
    "        if years is None:\n",
    "            years = VALID_YEARS\n",
    "            logger.warning(f\"Using default years: {years}\")\n",
    "\n",
    "        if isinstance(years, int):\n",
    "            years = [years]\n",
    "\n",
    "        # Validate all years are valid\n",
    "        invalid_years = [year for year in years if year not in VALID_YEARS]\n",
    "        if invalid_years:\n",
    "            logger.error(f\"Invalid years specified: {invalid_years}\")\n",
    "            raise ValueError(\n",
    "                f\"Invalid years: {invalid_years}. Valid years are {VALID_YEARS}\"\n",
    "            )\n",
    "\n",
    "        all_data: List[pd.DataFrame] = []\n",
    "\n",
    "        for year in tqdm(years):\n",
    "            logger.info(f\"Processing data for year {year}\")\n",
    "            try:\n",
    "                # Download zip file\n",
    "                logger.debug(f\"Downloading SCF data for year {year}\")\n",
    "                url = scf_url(year)\n",
    "                try:\n",
    "                    response = requests.get(url, timeout=60)\n",
    "                    response.raise_for_status()  # Raise an error for bad responses\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    logger.error(\n",
    "                        f\"Network error downloading SCF data for year {year}: {str(e)}\"\n",
    "                    )\n",
    "                    raise RuntimeError(\n",
    "                        f\"Failed to download SCF data for year {year}\"\n",
    "                    ) from e\n",
    "\n",
    "                # Process zip file\n",
    "                try:\n",
    "                    logger.debug(\"Creating zipfile from downloaded content\")\n",
    "                    z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "                    # Find the .dta file in the zip\n",
    "                    dta_files: List[str] = [\n",
    "                        f for f in z.namelist() if f.endswith(\".dta\")\n",
    "                    ]\n",
    "                    if not dta_files:\n",
    "                        logger.error(\n",
    "                            f\"No Stata files found in zip for year {year}\"\n",
    "                        )\n",
    "                        raise ValueError(\n",
    "                            f\"No Stata files found in zip for year {year}\"\n",
    "                        )\n",
    "\n",
    "                    logger.debug(f\"Found Stata files: {dta_files}\")\n",
    "\n",
    "                    # Read the Stata file\n",
    "                    try:\n",
    "                        logger.debug(f\"Reading Stata file: {dta_files[0]}\")\n",
    "                        with z.open(dta_files[0]) as f:\n",
    "                            df = pd.read_stata(\n",
    "                                io.BytesIO(f.read()), columns=columns\n",
    "                            )\n",
    "                            logger.debug(\n",
    "                                f\"Read DataFrame with shape {df.shape}\"\n",
    "                            )\n",
    "\n",
    "                        # Ensure 'wgt' is included\n",
    "                        if (\n",
    "                            columns is not None\n",
    "                            and \"wgt\" not in df.columns\n",
    "                            and \"wgt\" not in columns\n",
    "                        ):\n",
    "                            logger.debug(\"Re-reading with 'wgt' column added\")\n",
    "                            # Re-read to include weights\n",
    "                            with z.open(dta_files[0]) as f:\n",
    "                                cols_with_weight: List[str] = list(\n",
    "                                    set(columns) | {\"wgt\"}\n",
    "                                )\n",
    "                                df = pd.read_stata(\n",
    "                                    io.BytesIO(f.read()),\n",
    "                                    columns=cols_with_weight,\n",
    "                                )\n",
    "                                logger.debug(\n",
    "                                    f\"Re-read DataFrame with shape {df.shape}\"\n",
    "                                )\n",
    "                    except Exception as e:\n",
    "                        logger.error(\n",
    "                            f\"Error reading Stata file for year {year}: {str(e)}\"\n",
    "                        )\n",
    "                        raise RuntimeError(\n",
    "                            f\"Failed to process Stata file for year {year}\"\n",
    "                        ) from e\n",
    "\n",
    "                except zipfile.BadZipFile as e:\n",
    "                    logger.error(f\"Bad zip file for year {year}: {str(e)}\")\n",
    "                    raise RuntimeError(\n",
    "                        f\"Downloaded zip file is corrupt for year {year}\"\n",
    "                    ) from e\n",
    "\n",
    "                # Add year column\n",
    "                df[\"year\"] = year\n",
    "                logger.info(\n",
    "                    f\"Successfully processed data for year {year}, shape: {df.shape}\"\n",
    "                )\n",
    "                all_data.append(df)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing year {year}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        # Combine all years\n",
    "        logger.debug(f\"Combining data from {len(all_data)} years\")\n",
    "        if len(all_data) > 1:\n",
    "            result = pd.concat(all_data)\n",
    "            logger.info(\n",
    "                f\"Combined data from {len(years)} years, final shape: {result.shape}\"\n",
    "            )\n",
    "            return result\n",
    "        else:\n",
    "            logger.info(\n",
    "                f\"Returning data for single year, shape: {all_data[0].shape}\"\n",
    "            )\n",
    "            return all_data[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in _load: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f2e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [\n",
    "    \"hhsex\",  # sex of head of household\n",
    "    \"age\",  # age of respondent (hoh)\n",
    "    \"married\",  # marital status of respondent (hoh)\n",
    "    \"kids\",  # number of children in household\n",
    "    \"race\",  # race of respondent (hoh)\n",
    "    \"income\",  # total annual income of household\n",
    "    \"wageinc\",  # income from wages and salaries\n",
    "    \"bussefarminc\",  # income from business, self-employment or farm\n",
    "    \"intdivinc\",  # income from interest and dividends\n",
    "    \"ssretinc\",  # income from social security and retirement accounts\n",
    "]\n",
    "\n",
    "imputed_variables = [\"networth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3467f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "scf_data = load_scf(2022)\n",
    "scf_data = scf_data[predictors + imputed_variables]\n",
    "scf_marital_mapping = {\n",
    "    1: 1,  # Married - civilian spouse present -> Married\n",
    "    2: 5,  # Living with partner -> Never Married\n",
    "    3: 2,  # Separated -> Separated\n",
    "    4: 3,  # Divorced -> Divorced\n",
    "    5: 4,  # Widowed -> Widowed\n",
    "    6: 5,  # Never Married -> Never Married\n",
    "}\n",
    "\n",
    "# Apply the mapping to recode the marital status values\n",
    "scf_data[\"married\"] = np.vectorize(scf_marital_mapping.get)(\n",
    "    scf_data[\"married\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf8f334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hhsex', 'age', 'married', 'kids', 'race', 'income', 'wageinc', 'bussefarminc', 'intdivinc', 'ssretinc']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hhsex</th>\n",
       "      <th>age</th>\n",
       "      <th>married</th>\n",
       "      <th>kids</th>\n",
       "      <th>race</th>\n",
       "      <th>income</th>\n",
       "      <th>wageinc</th>\n",
       "      <th>bussefarminc</th>\n",
       "      <th>intdivinc</th>\n",
       "      <th>ssretinc</th>\n",
       "      <th>networth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38804.734469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.045591</td>\n",
       "      <td>38804.734469</td>\n",
       "      <td>762100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38264.278557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.045591</td>\n",
       "      <td>37183.366733</td>\n",
       "      <td>854300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>36102.454910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.045591</td>\n",
       "      <td>35021.543086</td>\n",
       "      <td>678200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>33508.266533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.045591</td>\n",
       "      <td>32427.354709</td>\n",
       "      <td>279600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>35561.998998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.045591</td>\n",
       "      <td>35561.998998</td>\n",
       "      <td>602600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>224829.659319</td>\n",
       "      <td>9836.297595</td>\n",
       "      <td>215101.452906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>721800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>224829.659319</td>\n",
       "      <td>9836.297595</td>\n",
       "      <td>215101.452906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>723800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>224829.659319</td>\n",
       "      <td>9944.388778</td>\n",
       "      <td>215101.452906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>721400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>224829.659319</td>\n",
       "      <td>9836.297595</td>\n",
       "      <td>215101.452906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>724300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>224829.659319</td>\n",
       "      <td>9836.297595</td>\n",
       "      <td>215101.452906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>722600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>445335.671343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>853.920341</td>\n",
       "      <td>326651.553106</td>\n",
       "      <td>76744.739479</td>\n",
       "      <td>43597590.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>461549.348697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>853.920341</td>\n",
       "      <td>321030.811623</td>\n",
       "      <td>95120.240481</td>\n",
       "      <td>44451510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>469115.731463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>853.920341</td>\n",
       "      <td>318577.141784</td>\n",
       "      <td>98362.975952</td>\n",
       "      <td>41056890.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>444254.759519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>853.920341</td>\n",
       "      <td>305465.681363</td>\n",
       "      <td>95120.240481</td>\n",
       "      <td>44730180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>502623.997996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>853.920341</td>\n",
       "      <td>381994.238477</td>\n",
       "      <td>72421.092184</td>\n",
       "      <td>43740180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152408.567134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12970.941884</td>\n",
       "      <td>139437.625251</td>\n",
       "      <td>2203000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>149165.831663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9512.024048</td>\n",
       "      <td>139437.625251</td>\n",
       "      <td>2006000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>145923.096192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6377.379760</td>\n",
       "      <td>139437.625251</td>\n",
       "      <td>2000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>145923.096192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6377.379760</td>\n",
       "      <td>139437.625251</td>\n",
       "      <td>2354000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>144842.184369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5404.559118</td>\n",
       "      <td>139437.625251</td>\n",
       "      <td>2000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hhsex  age  married  kids  race         income      wageinc  \\\n",
       "0       2   70        5     2     1   38804.734469     0.000000   \n",
       "1       2   70        5     2     1   38264.278557     0.000000   \n",
       "2       2   70        5     2     1   36102.454910     0.000000   \n",
       "3       2   70        5     2     1   33508.266533     0.000000   \n",
       "4       2   70        5     2     1   35561.998998     0.000000   \n",
       "5       1   46        5     0     1  224829.659319  9836.297595   \n",
       "6       1   46        5     0     1  224829.659319  9836.297595   \n",
       "7       1   46        5     0     1  224829.659319  9944.388778   \n",
       "8       1   46        5     0     1  224829.659319  9836.297595   \n",
       "9       1   46        5     0     1  224829.659319  9836.297595   \n",
       "10      1   68        1     0     1  445335.671343     0.000000   \n",
       "11      1   68        1     0     1  461549.348697     0.000000   \n",
       "12      1   68        1     0     1  469115.731463     0.000000   \n",
       "13      1   68        1     0     1  444254.759519     0.000000   \n",
       "14      1   68        1     0     1  502623.997996     0.000000   \n",
       "15      2   74        5     0     1  152408.567134     0.000000   \n",
       "16      2   74        5     0     1  149165.831663     0.000000   \n",
       "17      2   74        5     0     1  145923.096192     0.000000   \n",
       "18      2   74        5     0     1  145923.096192     0.000000   \n",
       "19      2   74        5     0     1  144842.184369     0.000000   \n",
       "\n",
       "     bussefarminc      intdivinc       ssretinc    networth  \n",
       "0        0.000000      54.045591   38804.734469    762100.0  \n",
       "1        0.000000      54.045591   37183.366733    854300.0  \n",
       "2        0.000000      54.045591   35021.543086    678200.0  \n",
       "3        0.000000      54.045591   32427.354709    279600.0  \n",
       "4        0.000000      54.045591   35561.998998    602600.0  \n",
       "5   215101.452906       0.000000       0.000000    721800.0  \n",
       "6   215101.452906       0.000000       0.000000    723800.0  \n",
       "7   215101.452906       0.000000       0.000000    721400.0  \n",
       "8   215101.452906       0.000000       0.000000    724300.0  \n",
       "9   215101.452906       0.000000       0.000000    722600.0  \n",
       "10     853.920341  326651.553106   76744.739479  43597590.0  \n",
       "11     853.920341  321030.811623   95120.240481  44451510.0  \n",
       "12     853.920341  318577.141784   98362.975952  41056890.0  \n",
       "13     853.920341  305465.681363   95120.240481  44730180.0  \n",
       "14     853.920341  381994.238477   72421.092184  43740180.0  \n",
       "15       0.000000   12970.941884  139437.625251   2203000.0  \n",
       "16       0.000000    9512.024048  139437.625251   2006000.0  \n",
       "17       0.000000    6377.379760  139437.625251   2000000.0  \n",
       "18       0.000000    6377.379760  139437.625251   2354000.0  \n",
       "19       0.000000    5404.559118  139437.625251   2000000.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictors)\n",
    "scf_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b87186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/movil1/Desktop/PYTHONJOBS/PolicyEngine/microimpute/docs/examples/data/Census CPS 2022.h5\"\n",
    "\n",
    "household_df = pd.read_hdf(file_path, key=\"household\")\n",
    "household_df = household_df.set_index(\"H_SEQ\")\n",
    "person_df = pd.read_hdf(file_path, key=\"person\")\n",
    "person_df = person_df.set_index(\"PH_SEQ\")\n",
    "cps_data = pd.DataFrame()\n",
    "cps_data[\"household_id\"] = household_df.index\n",
    "person_df[\"is_household_head\"] = person_df.P_SEQ == 1\n",
    "cps_data[\"hhsex\"] = (\n",
    "    person_df[person_df.is_household_head].loc[household_df.index].A_SEX.values\n",
    ")\n",
    "cps_data[\"age\"] = (\n",
    "    person_df[person_df.is_household_head].loc[household_df.index].A_AGE.values\n",
    ")\n",
    "cps_data[\"married\"] = (\n",
    "    person_df[person_df.is_household_head]\n",
    "    .loc[household_df.index]\n",
    "    .A_MARITL.values\n",
    ")\n",
    "person_df[\"is_child\"] = person_df.A_AGE < 18\n",
    "cps_data[\"kids\"] = (\n",
    "    person_df.groupby(person_df.index)\n",
    "    .is_child.sum()\n",
    "    .loc[household_df.index]\n",
    "    .values\n",
    ")\n",
    "cps_data[\"race\"] = (\n",
    "    person_df[person_df.is_household_head]\n",
    "    .loc[household_df.index]\n",
    "    .PRDTRACE.values\n",
    ")\n",
    "cps_data[\"income\"] = household_df.HTOTVAL.values\n",
    "cps_data[\"wageinc\"] = household_df.HWSVAL.values\n",
    "cps_data[\"bussefarminc\"] = (\n",
    "    household_df.HSEVAL.values + household_df.HFRVAL.values\n",
    ")\n",
    "cps_data[\"intdivinc\"] = (\n",
    "    household_df.HINTVAL.values + household_df.HDIVVAL.values\n",
    ")\n",
    "cps_data[\"ssretinc\"] = (\n",
    "    household_df.HSSVAL.values\n",
    "    + household_df.HSSIVAL.values\n",
    "    + household_df.HPENVAL.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ada8e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cps_marital_mapping = {\n",
    "    1: 1,  # Married - civilian spouse present -> Married\n",
    "    2: 1,  # Married - AF spouse present -> Married\n",
    "    3: 1,  # Married - spouse absent -> Married\n",
    "    4: 4,  # Widowed -> Widowed\n",
    "    5: 3,  # Divorced -> Divorced\n",
    "    6: 2,  # Separated -> Separated\n",
    "    7: 5,  # Never married -> Never married\n",
    "}\n",
    "\n",
    "# Apply the mapping to recode the marital status values\n",
    "cps_data[\"married\"] = np.vectorize(cps_marital_mapping.get)(\n",
    "    cps_data[\"married\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d8c8af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cps_race_mapping = {\n",
    "    1: 1,  # White only -> WHITE\n",
    "    2: 2,  # Black only -> BLACK/AFRICAN-AMERICAN\n",
    "    3: 5,  # American Indian, Alaskan Native only -> AMERICAN INDIAN/ALASKA NATIVE\n",
    "    4: 4,  # Asian only -> ASIAN\n",
    "    5: 6,  # Hawaiian/Pacific Islander only -> NATIVE HAWAIIAN/PACIFIC ISLANDER\n",
    "    6: 7,  # White-Black -> OTHER\n",
    "    7: 7,  # White-AI -> OTHER\n",
    "    8: 7,  # White-Asian -> OTHER\n",
    "    9: 7,  # White-HP -> OTHER\n",
    "    10: 7,  # Black-AI -> OTHER\n",
    "    11: 7,  # Black-Asian -> OTHER\n",
    "    12: 7,  # Black-HP -> OTHER\n",
    "    13: 7,  # AI-Asian -> OTHER\n",
    "    14: 7,  # AI-HP -> OTHER\n",
    "    15: 7,  # Asian-HP -> OTHER\n",
    "    16: 7,  # White-Black-AI -> OTHER\n",
    "    17: 7,  # White-Black-Asian -> OTHER\n",
    "    18: 7,  # White-Black-HP -> OTHER\n",
    "    19: 7,  # White-AI-Asian -> OTHER\n",
    "    20: 7,  # White-AI-HP -> OTHER\n",
    "    21: 7,  # White-Asian-HP -> OTHER\n",
    "    22: 7,  # Black-AI-Asian -> OTHER\n",
    "    23: 7,  # White-Black-AI-Asian -> OTHER\n",
    "    24: 7,  # White-AI-Asian-HP -> OTHER\n",
    "    25: 7,  # Other 3 race comb. -> OTHER\n",
    "    26: 7,  # Other 4 or 5 race comb. -> OTHER\n",
    "}\n",
    "\n",
    "# Apply the mapping to recode the race values\n",
    "cps_data[\"race\"] = np.vectorize(cps_race_mapping.get)(cps_data[\"race\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61360786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>household_id</th>\n",
       "      <th>hhsex</th>\n",
       "      <th>age</th>\n",
       "      <th>married</th>\n",
       "      <th>kids</th>\n",
       "      <th>race</th>\n",
       "      <th>income</th>\n",
       "      <th>wageinc</th>\n",
       "      <th>bussefarminc</th>\n",
       "      <th>intdivinc</th>\n",
       "      <th>ssretinc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50137</td>\n",
       "      <td>42000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>78</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14713</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>122446</td>\n",
       "      <td>107000</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>15344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44829</td>\n",
       "      <td>0</td>\n",
       "      <td>20000</td>\n",
       "      <td>205</td>\n",
       "      <td>24624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>91920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4500</td>\n",
       "      <td>71820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>68500</td>\n",
       "      <td>67000</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12843</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>75951</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1007</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>148514</td>\n",
       "      <td>70000</td>\n",
       "      <td>80000</td>\n",
       "      <td>1013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>825</td>\n",
       "      <td>14400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52644</td>\n",
       "      <td>35002</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>177585</td>\n",
       "      <td>118000</td>\n",
       "      <td>0</td>\n",
       "      <td>10193</td>\n",
       "      <td>41412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62643</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1006</td>\n",
       "      <td>58590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>210462</td>\n",
       "      <td>0</td>\n",
       "      <td>200000</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>79637</td>\n",
       "      <td>5760</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>40562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    household_id  hhsex  age  married  kids  race  income  wageinc  \\\n",
       "0              3      2   66        4     0     1   28920        0   \n",
       "1              4      2   52        1     0     1   50137    42000   \n",
       "2             15      2   78        4     0     1   14713        0   \n",
       "3             16      1   65        1     0     1  122446   107000   \n",
       "4             17      2   74        1     0     1   44829        0   \n",
       "5             18      1   76        1     0     1   91920        0   \n",
       "6             22      1   63        1     0     1   68500    67000   \n",
       "7             23      2   41        5     1     1    5953        0   \n",
       "8             24      2   52        1     0     1   14400        0   \n",
       "9             25      2   85        4     0     1   12843        0   \n",
       "10            27      1   71        5     0     1   75951        0   \n",
       "11            28      2   37        1     3     1  148514    70000   \n",
       "12            31      1   60        3     0     1   21242        0   \n",
       "13            32      1   57        1     0     1   15225        0   \n",
       "14            39      1   67        1     0     1   52644    35002   \n",
       "15            41      2   50        1     0     1   32403        0   \n",
       "16            57      1   69        1     1     1  177585   118000   \n",
       "17            61      2   76        3     0     1   62643        0   \n",
       "18            70      2   66        1     0     1  210462        0   \n",
       "19            73      1   85        1     0     1   79637     5760   \n",
       "\n",
       "    bussefarminc  intdivinc  ssretinc  \n",
       "0              0          0     26520  \n",
       "1              0          1      8136  \n",
       "2              0          1     14712  \n",
       "3              0        102     15344  \n",
       "4          20000        205     24624  \n",
       "5              0       4500     71820  \n",
       "6              0       1500         0  \n",
       "7              0          1         0  \n",
       "8              0          0     14400  \n",
       "9              0          2     12841  \n",
       "10             0       1007     50000  \n",
       "11         80000       1013         0  \n",
       "12             0          1     21241  \n",
       "13             0        825     14400  \n",
       "14             0          1     17641  \n",
       "15             0          3     32400  \n",
       "16             0      10193     41412  \n",
       "17             0       1006     58590  \n",
       "18        200000         10         0  \n",
       "19             0         51     40562  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c19660",
   "metadata": {},
   "source": [
    "## Running wealth imputation with autoimpute\n",
    "\n",
    "After harmonizing the two datasets, the `autoimpute` function from `microimpute` can be used to transfer wealth information from the SCF to the CPS. This powerful function streamlines the imputation process by automating hyperparameter tuning, method selection, validation, and application.\n",
    "\n",
    "Behind the scenes, `autoimpute` evaluates multiple statistical approaches, including Quantile Random Forest, Ordinary Least Squares, Quantile Regression, and Statistical Matching. It performs cross-validation to determine which method most accurately captures the relationship between the predictor variables and wealth measures in the SCF data. The function then applies the best-performing method to generate synthetic wealth values for CPS households.\n",
    "\n",
    "By enabling hyperparameter tuning, the function can optimize each method's parameters, further improving imputation accuracy. This automated approach saves considerable time and effort compared to manually testing different imputation strategies, while ensuring the selection of the most appropriate method for this specific imputation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7a96f83",
   "metadata": {
    "nbsphinx": {
    "allow_errors": true
  }},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7298c8731c9d48ee8ac2684ffb05b5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating models:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "/Users/movil1/anaconda3/envs/microimpute/lib/python3.11/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/movil1/anaconda3/envs/microimpute/lib/python3.11/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/movil1/anaconda3/envs/microimpute/lib/python3.11/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/movil1/anaconda3/envs/microimpute/lib/python3.11/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/movil1/anaconda3/envs/microimpute/lib/python3.11/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   37.8s remaining:   56.8s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   37.9s remaining:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   38.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.13751983642578125s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:    0.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "/Users/movil1/anaconda3/envs/microimpute/lib/python3.11/site-packages/statsmodels/regression/quantile_regression.py:191: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  warnings.warn(\"Maximum number of iterations (\" + str(max_iter) +\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    8.2s remaining:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:    9.0s remaining:    6.0s\n",
      "/Users/movil1/anaconda3/envs/microimpute/lib/python3.11/site-packages/statsmodels/regression/quantile_regression.py:191: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  warnings.warn(\"Maximum number of iterations (\" + str(max_iter) +\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   10.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  9.4min remaining: 14.0min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:  9.6min remaining:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  9.6min finished\n",
      "python(14522) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(14523) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(14524) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of receiver data before imputation: (56839, 11) \n",
      "Shape of receiver data after imputation: (56839, 12)\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Run the autoimpute process\n",
    "imputations, imputed_data, fitted_model, method_results_df = autoimpute(\n",
    "    donor_data=scf_data,\n",
    "    receiver_data=cps_data,\n",
    "    predictors=predictors,\n",
    "    imputed_variables=imputed_variables,\n",
    "    tune_hyperparameters=True,  # enable automated hyperparameter tuning\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Shape of receiver data before imputation: {cps_data.shape} \\nShape of receiver data after imputation: {imputed_data.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c2c1ad",
   "metadata": {},
   "source": [
    "## Comparing method performance\n",
    "\n",
    "The method comparison plot below shows how different imputation methods performed across various quantiles. Lower quantile loss values indicate better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38693461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the quantiles used in the evaluation\n",
    "quantiles = [q for q in method_results_df.columns if isinstance(q, float)]\n",
    "\n",
    "comparison_viz = method_comparison_results(\n",
    "    data=method_results_df,\n",
    "    metric_name=\"Test Quantile Loss\",\n",
    "    data_format=\"wide\",\n",
    ")\n",
    "fig = comparison_viz.plot(\n",
    "    title=\"Autoimpute Method Comparison\",\n",
    "    show_mean=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5409e8",
   "metadata": {},
   "source": [
    "![png](./scf_to_cps_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef14a3",
   "metadata": {},
   "source": [
    "## Evaluating wealth imputations\n",
    "\n",
    "To assess the imputation results, a comparison the distribution of wealth in the original SCF data with the imputed values in the CPS allow examining how well the imputation preserves important characteristics of the wealth distribution, such as its shape, central tendency, and dispersion.\n",
    "\n",
    "Wealth distributions are typically highly skewed, with a long right tail representing a small number of households with very high net worth. A successful imputation should preserve this characteristic skewness while maintaining realistic values across the entire distribution. Examining both the raw distributions and log-transformed versions of wealth values can better capture important information for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "236494f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_networth_distributions(\n",
    "    scf_data: pd.DataFrame,\n",
    "    imputed_data: pd.DataFrame,\n",
    ") -> go.Figure:\n",
    "    \"\"\"Plot the distribution of net worth in SCF and imputed CPS data.\n",
    "\n",
    "    Args:\n",
    "        scf_data: DataFrame containing SCF data.\n",
    "        imputed_data: DataFrame containing imputed CPS data.\n",
    "\n",
    "    Returns:\n",
    "        Plotly figure object.\n",
    "    \"\"\"\n",
    "    logger.info(\"Plotting net worth distributions\")\n",
    "\n",
    "    # 0. Check for missing values\n",
    "    if scf_data['networth'].isnull().any():\n",
    "        logger.warning(\"SCF data contains missing values in 'networth' column\")\n",
    "    if imputed_data['networth'].isnull().any():\n",
    "        logger.warning(\"Imputed data contains missing values in 'networth' column\")    \n",
    "    \n",
    "    # 1. Compute medians & means\n",
    "    scf_median = scf_data['networth'].median()\n",
    "    scf_mean   = scf_data['networth'].mean()\n",
    "    cps_median = imputed_data['networth'].median()\n",
    "    cps_mean   = imputed_data['networth'].mean()\n",
    "\n",
    "    # 2. Compute upper_limit & true max counts\n",
    "    upper_limit = np.percentile(scf_data['networth'], 99.5)\n",
    "    scf_counts, _ = np.histogram(scf_data['networth'], bins=150, range=(0,upper_limit))\n",
    "    cps_counts, _ = np.histogram(imputed_data['networth'], bins=150, range=(0,upper_limit))\n",
    "    max_scf = scf_counts.max()\n",
    "    max_cps = cps_counts.max()\n",
    "\n",
    "    # 3. Build subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=[\n",
    "            \"Distribution of Networth in SCF Data (Original)\",\n",
    "            \"Distribution of Imputed Networth in CPS Data\"\n",
    "        ],\n",
    "        vertical_spacing=0.15\n",
    "    )\n",
    "\n",
    "    # 4. Add histograms\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=scf_data['networth'],\n",
    "            nbinsx=150,\n",
    "            opacity=0.7,\n",
    "            marker_color='blue',\n",
    "            showlegend=False\n",
    "        ), row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=imputed_data['networth'],\n",
    "            nbinsx=150,\n",
    "            opacity=0.7,\n",
    "            marker_color='purple',\n",
    "            showlegend=False\n",
    "        ), row=2, col=1\n",
    "    )\n",
    "\n",
    "    # 5. Add full-height median/mean lines for SCF\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[scf_median, scf_median],\n",
    "            y=[0, 20000],\n",
    "            mode='lines',\n",
    "            line=dict(color='blue',   dash='dash'),\n",
    "            name=f\"Median: ${scf_median:,.0f}\",\n",
    "            legendgroup='scf'\n",
    "        ), row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[scf_mean, scf_mean],\n",
    "            y=[0, 20000],\n",
    "            mode='lines',\n",
    "            line=dict(color='blue', dash='dot'),\n",
    "            name=f\"Mean: ${scf_mean:,.0f}\",\n",
    "            legendgroup='scf'\n",
    "        ), row=1, col=1\n",
    "    )\n",
    "\n",
    "    # 6. Add full-height median/mean lines for CPS\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[cps_median, cps_median],\n",
    "            y=[0, 38000],\n",
    "            mode='lines',\n",
    "            line=dict(color='purple',   dash='dash'),\n",
    "            name=f\"Median: ${cps_median:,.0f}\",\n",
    "            legendgroup='cps'\n",
    "        ), row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[cps_mean, cps_mean],\n",
    "            y=[0, 38000],\n",
    "            mode='lines',\n",
    "            line=dict(color='purple', dash='dot'),\n",
    "            name=f\"Mean: ${cps_mean:,.0f}\",\n",
    "            legendgroup='cps'\n",
    "        ), row=2, col=1\n",
    "    )\n",
    "\n",
    "    # 7. Final layout tweaks\n",
    "    fig.update_layout(\n",
    "        title_text=\"Comparison of Original and Imputed Networth Distributions\",\n",
    "        height=PLOT_CONFIG[\"height\"],\n",
    "        width =PLOT_CONFIG[\"width\"],\n",
    "        legend_tracegroupgap=180\n",
    "    )\n",
    "\n",
    "    # 8. Zoom x-axis to drop extreme outliers\n",
    "    fig.update_xaxes(range=[0, upper_limit], row=1, col=1, title_text=\"Networth\")\n",
    "    fig.update_xaxes(range=[0, upper_limit], row=2, col=1, title_text=\"Networth\")\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b18f5",
   "metadata": {},
   "source": [
    "![png](./scf_to_cps_networth_dist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ca8d5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_log_transformed_distributions(\n",
    "    scf_data: pd.DataFrame,\n",
    "    imputed_data: pd.DataFrame,\n",
    ") -> go.Figure:\n",
    "    \"\"\"Plot the log-transformed distribution of net worth in SCF and imputed CPS data.\n",
    "    \n",
    "    Args:\n",
    "        scf_data: DataFrame containing SCF data.\n",
    "        imputed_data: DataFrame containing imputed CPS data.\n",
    "\n",
    "    Returns:\n",
    "        Plotly figure object.\n",
    "    \"\"\" \n",
    "    # Create a log transformation function that handles negative values\n",
    "    def safe_log(x):\n",
    "        # For negative values, take log of absolute value and negate\n",
    "        # For zero, replace with a small positive value\n",
    "        sign = np.sign(x)\n",
    "        log_x = np.log10(np.maximum(np.abs(x), 1e-10))\n",
    "        return sign * log_x\n",
    "\n",
    "    # Create log-transformed data\n",
    "    scf_log = safe_log(scf_data['networth'])\n",
    "    cps_log = safe_log(imputed_data['networth'])\n",
    "\n",
    "    # Calculate statistics for log-transformed data\n",
    "    scf_log_median = np.median(scf_log)\n",
    "    cps_log_median = np.median(cps_log)\n",
    "    scf_log_mean = np.mean(scf_log)\n",
    "    cps_log_mean = np.mean(cps_log)\n",
    "\n",
    "    # Create a single plot with both distributions\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add histograms for both datasets\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=scf_log,\n",
    "            nbinsx=60,\n",
    "            opacity=0.7,\n",
    "            name=\"SCF Log Networth\",\n",
    "            marker_color='blue'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=cps_log,\n",
    "            nbinsx=60,\n",
    "            opacity=0.7,\n",
    "            name=\"CPS Imputed Log Networth\",\n",
    "            marker_color='purple'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add vertical lines for medians\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[scf_log_median, scf_log_median],\n",
    "            y=[0, 10000],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"blue\", width=2, dash=\"dash\"),\n",
    "            name=f\"SCF Median: ${10**scf_log_median:,.0f}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[cps_log_median, cps_log_median],\n",
    "            y=[0, 10000],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"purple\", width=2, dash=\"dash\"),\n",
    "            name=f\"CPS Median: ${10**cps_log_median:,.0f}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add vertical lines for means\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[scf_log_mean, scf_log_mean],\n",
    "            y=[0, 10000],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"blue\", width=2, dash=\"dot\"),\n",
    "            name=f\"SCF Mean: ${10**scf_log_mean:,.0f}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[cps_log_mean, cps_log_mean],\n",
    "            y=[0, 10000],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"purple\", width=2, dash=\"dot\"),\n",
    "            name=f\"CPS Mean: ${10**cps_log_mean:,.0f}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout with improved titles and labels\n",
    "    fig.update_layout(\n",
    "        title=\"Log-Transformed Networth Distribution Comparison\",\n",
    "        xaxis_title=\"Log10 of Networth\",\n",
    "        yaxis_title=\"Frequency\",\n",
    "        height=PLOT_CONFIG[\"height\"],\n",
    "        width=PLOT_CONFIG[\"width\"], \n",
    "        barmode='overlay',\n",
    "        bargap=0.1,\n",
    "        legend=dict(\n",
    "            x=0.01,\n",
    "            y=0.99,\n",
    "            bgcolor=\"rgba(255, 255, 255, 0.8)\",\n",
    "            bordercolor=\"rgba(0, 0, 0, 0.3)\",\n",
    "            borderwidth=1,\n",
    "            orientation=\"v\",\n",
    "            xanchor=\"left\",\n",
    "            yanchor=\"top\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add tick labels showing the actual dollar values\n",
    "    tick_values = [-6, -4, -2, 0, 2, 4, 6, 8]\n",
    "    tick_labels = ['$' + format(10**x if x >= 0 else -10**abs(x), ',.0f') for x in tick_values]\n",
    "    fig.update_xaxes(\n",
    "        tickvals=tick_values,\n",
    "        ticktext=tick_labels\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710da2d0",
   "metadata": {},
   "source": [
    "![png](./scf_to_cps_networth_log_transform.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab616c4",
   "metadata": {},
   "source": [
    "The logarithmic transformation provides a clearer view of the wealth distribution across its entire range. By logarithmically scaling the data, the extreme values are compressed while expanding the visibility of differences in the lower and middle portions of the distribution.\n",
    "\n",
    "This transformation is particularly valuable for wealth data, where values can span many orders of magnitude. The plot above, shows how closely the imputed CPS wealth distribution matches the original SCF distribution in terms of shape and central tendency. The vertical lines marking the mean and median values help gauge how these statistical properties have been preserved through the imputation process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microimpute",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
