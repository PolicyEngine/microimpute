{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a New Imputer Model\n",
    "\n",
    "This notebook demonstrates how to create a new imputation model by extending the `Imputer` and `ImputerResults` abstract base classes in MicroImpute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the MicroImpute Architecture\n",
    "\n",
    "MicroImpute uses a two-class architecture for imputation models:\n",
    "\n",
    "1. **Imputer**: The base model class that handles model initialization and fitting\n",
    "2. **ImputerResults**: Represents a fitted model and handles prediction\n",
    "\n",
    "This separation provides a clean distinction between the model definition and the fitted model instance, similar to statsmodels' approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Any\n",
    "from pydantic import validate_call\n",
    "\n",
    "from us_imputation_benchmarking.models.imputer import Imputer, ImputerResults\n",
    "from us_imputation_benchmarking.config import VALIDATE_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing a Model Results Class\n",
    "\n",
    "First, we need to implement the `ImputerResults` subclass that will represent our fitted model and handle predictions. Let's create a model-specific imputer results class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewModelResults(ImputerResults):\n",
    "    \"\"\"\n",
    "    Fitted Model imputer ready for prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        predictors: List[str],\n",
    "        imputed_variables: List[str],\n",
    "        **kwargs: Dict[str, Any],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the NewModelResults parameter.\n",
    "\n",
    "        Args:\n",
    "            predictors: List of predictor variable names\n",
    "            imputed_variables: List of imputed variable names\n",
    "            **kwargs: Additional keyword arguments for model parameters\n",
    "        \"\"\"\n",
    "        super().__init__(predictors, imputed_variables)\n",
    "        # Add any additional model specific parameters here\n",
    "\n",
    "    # You may choose to validate your model parameters with pydantic\n",
    "    @validate_call(config=VALIDATE_CONFIG)\n",
    "    def _predict(\n",
    "        self, X_test: pd.DataFrame, quantiles: Optional[List[float]] = None\n",
    "    ) -> Dict[float, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Predict imputed values at specified quantiles.\n",
    "\n",
    "        Args:\n",
    "            X_test: DataFrame containing the test data\n",
    "            quantiles: List of quantiles to predict. If None, predicts at median\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping quantiles to DataFrames with predicted values\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If prediction fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Implement model specific prediction functionality...\n",
    "\n",
    "            return\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during Model prediction: {str(e)}\")\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to predict with Model: {str(e)}\"\n",
    "            ) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing the Main Model Class\n",
    "\n",
    "Next, let's implement the main `Imputer` subclass that will handle model initialization and fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewModel(Imputer):\n",
    "    \"\"\"\n",
    "    Imputation model to be fitted.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the model parameters.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    @validate_call(config=VALIDATE_CONFIG)\n",
    "    def _fit(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        predictors: List[str],\n",
    "        imputed_variables: List[str],\n",
    "        **kwargs: Any,\n",
    "    ) -> NewModelResults:\n",
    "        \"\"\"\n",
    "        Fit the Model on training data.\n",
    "\n",
    "        Args:\n",
    "            X_train: DataFrame containing training data\n",
    "            predictors: List of predictor variable names\n",
    "            imputed_variables: List of variable names to impute\n",
    "            **kwargs: Additional arguments passed specific to Model\n",
    "\n",
    "        Returns:\n",
    "            NewModelResults instance with the fitted model\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If model fitting fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Implement model specific training functionality...\n",
    "\n",
    "            # Return the results object with fitted models\n",
    "            return NewModelResults(\n",
    "                predictors=predictors,\n",
    "                imputed_variables=imputed_variables,\n",
    "                **kwargs,  # Pass any additional model parameters here\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error fitting Model: {str(e)}\")\n",
    "            raise RuntimeError(f\"Failed to fit Model: {str(e)}\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the New Model\n",
    "\n",
    "You can test the functionality of your newly implemented `NewModel` imputer model with a simple example using the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from us_imputation_benchmarking.comparisons.data import preprocess_data\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Define predictors and variables to impute\n",
    "predictors = [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\"]\n",
    "imputed_variables = [\"petal width (cm)\"]\n",
    "\n",
    "# Filter the data\n",
    "data = iris_df[predictors + imputed_variables]\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test = preprocess_data(data)\n",
    "\n",
    "# Initialize our new model\n",
    "new_imputer = NewModel()\n",
    "\n",
    "# Fit the model\n",
    "fitted_model = new_imputer.fit(\n",
    "    X_train,\n",
    "    predictors,\n",
    "    imputed_variables,\n",
    ")\n",
    "\n",
    "# Make predictions at different quantiles\n",
    "test_quantiles = [0.1, 0.5, 0.9]\n",
    "predictions = fitted_model.predict(X_test, test_quantiles)\n",
    "\n",
    "# Print sample predictions\n",
    "for q in test_quantiles:\n",
    "    print(f\"\\nPredictions at {q} quantile:\")\n",
    "    print(predictions[q].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integrating with the Benchmarking Framework\n",
    "\n",
    "The new `NewModel` model is then ready to be integrated into the MicroImpute benchmarking framework. Here's how you would compare it with other models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from us_imputation_benchmarking.models import OLS, QRF\n",
    "from us_imputation_benchmarking.comparisons import (\n",
    "    get_imputations,\n",
    "    compare_quantile_loss,\n",
    ")\n",
    "from us_imputation_benchmarking.comparisons.plot import plot_loss_comparison\n",
    "\n",
    "# Define models to compare\n",
    "model_classes = [NewModel, OLS, QRF]\n",
    "\n",
    "# Get test data for evaluation\n",
    "Y_test = X_test[imputed_variables]\n",
    "\n",
    "# Get imputations from all models\n",
    "method_imputations = get_imputations(\n",
    "    model_classes, X_train, X_test, predictors, imputed_variables\n",
    ")\n",
    "\n",
    "# Compare quantile loss\n",
    "loss_comparison_df = compare_quantile_loss(Y_test, method_imputations)\n",
    "\n",
    "# Plot the comparison\n",
    "plot_loss_comparison(loss_comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices for Implementing New Models\n",
    "\n",
    "When implementing a new imputation model for MicroImpute, follow these best practices:\n",
    "\n",
    "### Architecture\n",
    "\n",
    "1. **Create two classes**:\n",
    "   - An `Imputer` subclass for model definition and fitting\n",
    "   - An `ImputerResults` subclass for the fitted model and prediction\n",
    "\n",
    "2. **Implement required methods**:\n",
    "   - `_fit()` in your `Imputer` subclass\n",
    "   - `_predict()` in your `ImputerResults` subclass\n",
    "\n",
    "### Error Handling\n",
    "\n",
    "1. **Use thorough error handling**:\n",
    "   - Wrap model fitting and prediction in try/except blocks\n",
    "   - Provide informative error messages\n",
    "   - Use proper error types (ValueError, RuntimeError)\n",
    "\n",
    "2. **Log important information**:\n",
    "   - Use self.logger for consistent logging\n",
    "   - Log progress, parameters, and errors\n",
    "\n",
    "### Parameters and Validation\n",
    "\n",
    "1. **Use proper type hints**:\n",
    "   - Add type hints to all methods and parameters\n",
    "   - Use `validate_call` decorator on model methods (with VALIDATE_CONFIG)\n",
    "\n",
    "2. **Support model-specific parameters**:\n",
    "   - Design your `_fit()` and `_predict()` methods to accept model-specific parameters that will ensure their correct functionality\n",
    "   - Document parameters clearly in docstrings\n",
    "\n",
    "### Documentation\n",
    "\n",
    "1. **Add comprehensive docstrings**:\n",
    "   - Include class-level docstrings explaining the model's approach\n",
    "   - Document all methods with proper Args, Returns, and Raises sections\n",
    "   - Provide examples if helpful\n",
    "\n",
    "2. **Create unit tests**:\n",
    "   - Test both basic interface compliance and model-specific functionality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
